import re
import unicodedata
import pandas as pd
from sqlalchemy import text
from multiprocessing import Pool, cpu_count
from db_config import get_db_connection

# -------------------------------
# Fast fuzzy similarity
# -------------------------------
try:
    from rapidfuzz import fuzz, process
    def sim(a, b): return fuzz.token_set_ratio(a or '', b or '') / 100
except:
    import difflib
    def sim(a, b): return difflib.SequenceMatcher(None, (a or '').lower(), (b or '').lower()).ratio()

# -------------------------------
# Normalization helpers
# -------------------------------
STOP_WORDS = {
    # common postal suffixes / noise in India Post data
    "division", "city", "district", "region", "south", "north", "east", "west",
    "so", "s.o", "h.o", "bo", "b.o", "sub", "office", "post", "p.o", "po", "nodal",
    "rms", "ho", "do", "co", "branch", "branches", "central", "rural", "urban",
    "taluk", "taluka", "mandal", "zone"
}

# common alias map (short list; you can expand as you like)
CITY_ALIASES = {
    "bombay": "mumbai",
    "bengaluru": "bangalore",
    "bengluru": "bangalore",
    "benglore": "bangalore",
    "calcutta": "kolkata",
    "trivandrum": "thiruvananthapuram",
    "kochi": "cochin",
    "cochin": "kochi",
    "vadodara": "baroda",
    "baroda": "vadodara",
    "pondicherry": "puducherry"
}

def strip_accents(s: str) -> str:
    return ''.join(ch for ch in unicodedata.normalize('NFKD', s) if not unicodedata.combining(ch))

def basic_clean(s: str) -> str:
    s = strip_accents(str(s or '')).lower()
    s = re.sub(r'[\(\)\[\]\{\}\.,\-/_:;#@&]*', ' ', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def drop_stop_words(s: str) -> str:
    toks = [t for t in s.split() if t not in STOP_WORDS and not t.isdigit()]
    return ' '.join(toks).strip()

def normalize_city_like(s: str) -> str:
    s = basic_clean(s)
    s = drop_stop_words(s)
    # apply aliases
    toks = []
    for t in s.split():
        toks.append(CITY_ALIASES.get(t, t))
    return ' '.join(toks).strip().title()

def normalize_state_name(state_name, state_to_abbr, abbr_to_state):
    if pd.isna(state_name) or state_name is None:
        return None
    s = basic_clean(str(state_name)).replace('.', '').upper()
    # try abbreviation first
    if s in abbr_to_state:
        return abbr_to_state[s]
    # title case compare (e.g., "Maharashtra")
    st = s.title()
    # if full state name present in mapping, return title
    if st in state_to_abbr:
        return st
    return st

def extract_pin(txt):
    if not txt:
        return None
    m = re.search(r'(?<!\d)(\d{6})(?!\d)', str(txt))
    return m.group(1) if m else None

# -------------------------------
# Core scoring (city/state/office)
# -------------------------------
def score_candidate(addr_clean, city_in_norm, state_in_norm, cand_city_norm, cand_state_norm, office_norm):
    """
    Weighted score:
      - City similarity (after cleansing) has highest weight
      - State similarity secondary
      - Office/locality vs full address gives extra boost when locality text is present
    """
    city_score = sim(cand_city_norm, city_in_norm)
    state_score = sim(cand_state_norm, state_in_norm)
    locality_score = sim(office_norm, addr_clean)  # compare office/locality against full address text

    # You asked for high accuracy; these weights bias toward city, then state, then locality
    total = 0.58 * city_score + 0.27 * state_score + 0.15 * locality_score
    return city_score, state_score, locality_score, total

# -------------------------------
# One-record validator
# -------------------------------
def validate_record(record, master_by_pin, state_to_abbr, abbr_to_state):
    try:
        # Build Address1 = address1 + address2 + address3 (your requirement)
        addr_raw = " ".join([str(x) for x in [record.get('address1'), record.get('address2'), record.get('address3')]
                             if x and str(x).lower() != 'nan']).strip()
        addr_clean = basic_clean(addr_raw)

        pin = extract_pin(addr_raw) or extract_pin(record.get('pincode'))
        city_in_norm = normalize_city_like(record.get('city'))
        state_in_norm = normalize_state_name(record.get('state'), state_to_abbr, abbr_to_state)

        country = "India"
        country_conf = 1.0

        # Default outputs
        best_city = city_in_norm
        best_state = state_in_norm
        best_loc = None
        city_conf = state_conf = loc_conf = 0.0
        total_score = 0.0
        pin_match = "No"
        flag = "No"
        confidence_level = "Low"

        # ---------- PIN path ----------
        if pin:
            dfp = master_by_pin.get(pin)
            if dfp is None or dfp.empty:
                # PIN not in DB -> flag
                flag = "Yes"
            else:
                # Clean master fields (cache columns to avoid repeated work)
                if 'city_norm' not in dfp.columns:
                    dfp = dfp.copy()
                    dfp['city_norm'] = dfp['city'].apply(normalize_city_like)
                    dfp['state_norm'] = dfp['state'].apply(lambda s: normalize_state_name(s, state_to_abbr, abbr_to_state))
                    dfp['office_norm'] = dfp['office_name'].apply(normalize_city_like)
                    # return back to dict
                    master_by_pin[pin] = dfp

                # Score all candidates for this pincode
                scores = []
                for _, cand in dfp.iterrows():
                    ccs, scs, lcs, tot = score_candidate
